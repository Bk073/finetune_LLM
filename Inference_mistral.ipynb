{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6adb4f31-4a8d-47bf-a3e9-18c723f051aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DefaultDataCollator, TrainingArguments, Trainer, BitsAndBytesConfig, GenerationConfig\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
    "import evaluate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bafa09e4-bbc4-4b86-9925-842fc7917e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain==0.0.335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51df4ad1-ef7d-4f2d-b536-58086b0e6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai==1.3.0\n",
    "# !pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833a322e-2871-43d3-8e9a-b100fb8a179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e607fbf-dc26-4c17-9d7f-c228e8aa3324",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8c49601-6c38-4016-a0f8-5bb480078bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new data 2024_04_05\n",
    "train_dataset = load_dataset(\"csv\", data_files=\"./total_data_2023_04_05.csv\", split=\"train[:90%]\")\n",
    "eval_dataset = load_dataset(\"csv\", data_files=\"./total_data_2023_04_05.csv\", split=\"train[90%:]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f615ca4-4adf-4af8-88d3-48b130477157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=1024,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "def generate_prompt(data_point):\n",
    "    full_prompt = f\"\"\"<s>[INST]{data_point['instruction']}\n",
    "    {f\"Here is some context: {data_point['context']}\" if len(data_point[\"context\"]) > 0 else None}\n",
    "    [/INST] {data_point['response']}\n",
    "    </s>\"\"\"\n",
    "    return {\"text\": full_prompt}\n",
    "\n",
    "# def generate_prompt_two(data_point):\n",
    "#     full_prompt = f\"\"\"<s>[INST]### Instruction:{data_point['instruction']}\n",
    "#     {f\"\\n\\n Here is some context: ### Input:\" {data_point['context']}\" if len(data_point[\"context\"]) > 0 else None}\n",
    "#     [/INST]\\n\\n completion: {data_point['response']}\n",
    "#     </s>\"\"\"\n",
    "#     return {\"text\": full_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46584706-68ed-47e0-91dc-d79b56aa943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatting_func(example):\n",
    "  if example.get(\"context\", \"\") != \"\":\n",
    "      input_prompt = (f\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "      \"### Instruction:\\n\"\n",
    "      f\"{example['instruction']}\\n\\n\"\n",
    "      f\"### Input: \\n\"\n",
    "      f\"{example['context']}\\n\\n\"\n",
    "      f\"### Response: \\n\"\n",
    "      f\"{example['response']}\")\n",
    "\n",
    "  else:\n",
    "    input_prompt = (f\"Below is an instruction that describes a task. \"\n",
    "      \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "      \"### Instruction:\\n\"\n",
    "      f\"{example['instruction']}\\n\\n\"\n",
    "      f\"### Response:\\n\"\n",
    "      f\"{example['response']}\")\n",
    "\n",
    "  return {\"text\" : input_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b0fce33-687d-489a-86ee-affb2ab932b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_train_dataset = train_dataset.map(formatting_func, remove_columns=list(train_dataset.features))\n",
    "generated_eval_dataset = eval_dataset.map(formatting_func, remove_columns=list(eval_dataset.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "283cbb4e-1fe5-4238-bdf4-ef0173f358d5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7640c7f4-844c-4476-a945-93d41c5fe637",
   "metadata": {},
   "source": [
    "### Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e217c9e0-e1a5-4af0-b9ad-6f035e0d7f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  2.95s/it]\n"
     ]
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    # quantization_config = bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # use_auth_token=True\n",
    ")\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    "    padding_size=\"left\"\n",
    ")\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token\n",
    "# eval_tokenizer.add_eos_token=True\n",
    "# eval_tokenizer.add_bos_token, eval_tokenizer.add_eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a41f14f7-47a5-4fb0-a325-87d030ec2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(instruction, context = None):\n",
    "  if context:\n",
    "    prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "  else:\n",
    "    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "  \n",
    "  # model_input = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n",
    "  input_ids = eval_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "  base_model.eval()\n",
    "  with torch.no_grad():\n",
    "      return eval_tokenizer.decode(base_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=50, num_beams=1, pad_token_id=2))[0], \n",
    "                            skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66a1c11-fcd0-40c3-8b2a-468d993ae69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['instruction'] = []\n",
    "output['context'] = []\n",
    "output['response'] = []\n",
    "output['model_output'] = []\n",
    "for data in eval_dataset:\n",
    "    out = inference(data['instruction'], data['context'])\n",
    "    output['instruction'].append(data['instruction'])\n",
    "    output['context'].append(data['context'])\n",
    "    output['response'].append(data['response'])\n",
    "    output['model_output'].append(out.split('Response: \\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f60a25d6-87e8-4b1c-8745-803cfce2ad53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.07218230010375547, 'rouge2': 0.041823822482589236, 'rougeL': 0.0593621444294312, 'rougeLsum': 0.05981011588193556}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=output['model_output'], references=output['response'][0:len(output['model_output'])],\n",
    "                        use_aggregator=True,\n",
    "                        use_stemmer=True,)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828f317-6b92-4bf4-b283-fe4a893dfd53",
   "metadata": {},
   "source": [
    "### Finetuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1643a7bb-5d50-45c7-a5de-e4fc64a8694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_model = PeftModel.from_pretrained(base_model, \"./peft-training-1712523322/checkpoint-1005\", is_trainable=False, torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb4a873c-f649-44d9-8ad3-5572ec4cf18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, instruction , context = None):\n",
    "  if context:\n",
    "    prompt = f\"Below is an instruction that describes a task, paired with an input that provides further context.\\n\\n### Instruction: \\n{instruction}\\n\\n### Input: \\n{context}\\n\\n### Response: \\n\"\n",
    "  else:\n",
    "    prompt = f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction: \\n{instruction}\\n\\n### Response: \\n\"\n",
    "  \n",
    "  # model_input = tokenizer(prompt, return_tensors=\"pt\", return_token_type_ids=False).to(\"cuda\")\n",
    "  input_ids = eval_tokenizer(prompt, return_tensors=\"pt\").input_ids.to(\"cuda\")\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      return eval_tokenizer.decode(model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=50, num_beams=1, pad_token_id=2))[0], \n",
    "                            skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0129cd9b-9411-44e9-998c-9130eff4b80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['instruction'] = []\n",
    "output['context'] = []\n",
    "output['response'] = []\n",
    "output['model_output'] = []\n",
    "for data in eval_dataset:\n",
    "    out = inference(ft_model, data['instruction'], data['context'])\n",
    "    output['instruction'].append(data['instruction'])\n",
    "    output['context'].append(data['context'])\n",
    "    output['response'].append(data['response'])\n",
    "    output['model_output'].append(out.split('Response: \\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7aa2300-51bc-45cd-8ba7-6b5bc2f42fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.37572347736798006, 'rouge2': 0.2162117457770065, 'rougeL': 0.3064643618441903, 'rougeLsum': 0.30663628941332355}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=output['model_output'], references=output['response'][0:len(output['model_output'])],\n",
    "                        use_aggregator=True,\n",
    "                        use_stemmer=True,)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002d909e-a0cf-4f9e-ac93-7f4eb2075872",
   "metadata": {},
   "source": [
    "## OpenAI Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc6405b6-c896-4e4b-84e7-fda94eb79ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade openai\n",
    "# !pip install -U openai\n",
    "# !pip install openai\n",
    "# !pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19799a1d-4762-4252-803a-22429cb57cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "# import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f9ddbeff-29ec-424b-b20e-0b20f5f70181",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "def setup():\n",
    "    sys.path.append('./content')\n",
    "    os.environ[\"OPENAI_API_KEY\"] = ''\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "    openai.api_key  = os.environ['OPENAI_API_KEY']\n",
    "setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "651b7313-acbb-446e-abcf-e5dfc74719f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' How can I reduce my risk of developing complications from an infection during pregnancy?\\n\\n'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample = next(iter(eval_dataset))\n",
    "data_sample['instruction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "adc594ea-b807-4485-bdd2-3e3c11862bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "\n",
    "def inference_openai(instruction , context = None):\n",
    "    completion = client.chat.completions.create(\n",
    "      model=\"gpt-3.5-turbo\",\n",
    "      messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Below is an instruction that describes a task, paired with an input that provides further context.\"},\n",
    "        {\"role\": \"user\", \"content\": instruction + ' '+ context}\n",
    "      ]\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2cbe279-fa5e-4599-9c6b-d566f3ffebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['instruction'] = []\n",
    "output['context'] = []\n",
    "output['response'] = []\n",
    "output['model_output'] = []\n",
    "for data in eval_dataset:\n",
    "    out = inference_openai(data['instruction'], data['context'])\n",
    "    output['instruction'].append(data['instruction'])\n",
    "    output['context'].append(data['context'])\n",
    "    output['response'].append(data['response'])\n",
    "    output['model_output'].append(out.split('Response: \\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ca560ef-904e-4176-9ac7-b2eae25e4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.41020469809302945, 'rouge2': 0.22515338947438995, 'rougeL': 0.3063812409360829, 'rougeLsum': 0.31903575248890903}\n"
     ]
    }
   ],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=output['model_output'], references=output['response'][0:len(output['model_output'])],\n",
    "                        use_aggregator=True,\n",
    "                        use_stemmer=True,)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff21758-59d3-4b5e-945b-3bb794c0284d",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a56082d-cde4-4ac1-a77f-9ae9fa07aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, TokenTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings\n",
    "from langchain.embeddings import LlamaCppEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "# from llamaapi import LlamaAPI\n",
    "\n",
    "# from langchain.llms import LlamaCPP\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.chains import LLMChain\n",
    "import os\n",
    "\n",
    "from langchain.llms import LlamaCpp  \n",
    "from langchain.prompts import PromptTemplate  \n",
    "from langchain.chains import LLMChain  \n",
    "from langchain.callbacks.manager import CallbackManager  \n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler \n",
    "from os import path\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e4f44a6a-3174-4c78-aa69-6711693a0d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['context'] = []\n",
    "output['response'] = []\n",
    "\n",
    "for data in eval_dataset:\n",
    "    output['context'].append(data['context'])\n",
    "    output['response'].append(data['response'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ba26112-57fd-4ba7-ac75-f6c286a73f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spore) is causing the infection • Whether you ...</td>\n",
       "      <td>By taking proactive measures such as frequent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of sperm with morpho - logic abnormalities [16...</td>\n",
       "      <td>Fathers play a significant role in shaping th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>normal from abnormal,soobservationstendtobecat...</td>\n",
       "      <td>Low birth weight (LBW) refers to infants who ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>washcloth on your face helps ease tension and ...</td>\n",
       "      <td>Using a birthing ball can help decrease disco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>of these studies was that of Chanarin and Roth...</td>\n",
       "      <td>The study by Chanarin and Rothman suggests th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  spore) is causing the infection • Whether you ...   \n",
       "1  of sperm with morpho - logic abnormalities [16...   \n",
       "2  normal from abnormal,soobservationstendtobecat...   \n",
       "3  washcloth on your face helps ease tension and ...   \n",
       "4  of these studies was that of Chanarin and Roth...   \n",
       "\n",
       "                                            response  \n",
       "0   By taking proactive measures such as frequent...  \n",
       "1   Fathers play a significant role in shaping th...  \n",
       "2   Low birth weight (LBW) refers to infants who ...  \n",
       "3   Using a birthing ball can help decrease disco...  \n",
       "4   The study by Chanarin and Rothman suggests th...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(output)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2d103cc2-0fb2-4049-b14a-4b8752cc95f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('nutrition_pred_04_05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9145aac-83c1-48dd-ac35-644b71a87ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = CSVLoader(file_path='nutrition_pred_04_05.csv', source_column=\"context\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cb677066-0b27-48a0-9350-cd801d5c74da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=': 0\\ncontext: spore) is causing the infection • Whether you have antibodies to the organism from a prior exposure • Whether the disease is treatable • When during pregnancy you acquired the infection Even if you get an infection during pregnancy, your baby might not become infected—and even if your baby gets infected, he might not be harmed. The chart on pages 132–133 identiﬁes infections that are harmful during pregnancy, and the following sections provide information on the most serious of them. Ways to Avoid Getting Sick The best way to prevent complications from an infection is to avoid getting sick. Here are a few guidelines to follow: 1. Wash your hands several times each day, especially before eating and after using the toilet. Germs live on doorknobs, handrails, phones, hands, and other surfaces. After touching a germ-covered surface with your hands, you transmit the germs to your food, mouth, nose, and anything else you touch. 2. Stay away from sick people as much as possible, especially if your vaccinations aren’t up to date. 3. Update your vaccinations or have your immune status checked before pregnancy, if possible. Health care professionals recommend that women avoid certain vaccines—such as chicken pox (varicella zoster)\\nresponse: By taking proactive measures such as frequent handwashing, avoiding close contact with sick individuals, and ensuring your vaccinations are up to date, you can significantly lower your risk of experiencing complications from an infection during pregnancy. Additionally, maintaining a healthy lifestyle throughout pregnancy can also help boost your immune system and reduce the likelihood of developing complications.', metadata={'source': 'spore) is causing the infection • Whether you have antibodies to the organism from a prior exposure • Whether the disease is treatable • When during pregnancy you acquired the infection Even if you get an infection during pregnancy, your baby might not become infected—and even if your baby gets infected, he might not be harmed. The chart on pages 132–133 identiﬁes infections that are harmful during pregnancy, and the following sections provide information on the most serious of them. Ways to Avoid Getting Sick The best way to prevent complications from an infection is to avoid getting sick. Here are a few guidelines to follow: 1. Wash your hands several times each day, especially before eating and after using the toilet. Germs live on doorknobs, handrails, phones, hands, and other surfaces. After touching a germ-covered surface with your hands, you transmit the germs to your food, mouth, nose, and anything else you touch. 2. Stay away from sick people as much as possible, especially if your vaccinations aren’t up to date. 3. Update your vaccinations or have your immune status checked before pregnancy, if possible. Health care professionals recommend that women avoid certain vaccines—such as chicken pox (varicella zoster)', 'row': 0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e092211-94e3-4ba3-a281-5fa910306082",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1805"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a3a277d-ae77-4e1b-8832-5fa6e762431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "91ba69a0-7796-4478-8909-f477eb992cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_directory = 'docs/chroma/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3baa6194-e3a6-44f5-9434-cabe87afcdd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3031\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=embeddings\n",
    ")\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ada3b8f0-2510-476e-9603-bb2c952d303b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51e41dcd-f5ae-42ac-a7b3-574fd14d4bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(llm, vectordb, question):\n",
    "    template_2 = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "        \\n\\n###Instruction: {question}\n",
    "        \\n\\n### Input: {context}\n",
    "        \\n\\n### Response: \\n\"\"\"\n",
    "   \n",
    "    QA_CHAIN_PROMPT_2 = PromptTemplate.from_template(template_2)\n",
    "    qa_chain_2 = RetrievalQA.from_chain_type(\n",
    "        llm,\n",
    "        retriever=vectordb.as_retriever(),\n",
    "        return_source_documents=True,\n",
    "        chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT_2}\n",
    "    )\n",
    "    result_p_2= qa_chain_2({\"query\":question})\n",
    "    # return result_p_2['result']\n",
    "    return result_p_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d46a139f-951a-4384-bd49-f09c0fb4ee1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n How can I tell if my baby is getting enough movement in the womb?\\n\\n\\n### Input: \\nIf you have bright red bleeding of more than a spot or two at any time this month, call your care provider right away . It could be a sign of placental abruption, a serious problem in which your placenta separates from the wall of your uterus. This condition is a medical emer gency . However , try not to confuse this kind of bleeding with the slight bleeding you may have after a pelvic exam or with the blood and mucus you may see as the cervix thins. Constant, severe abdominal pain If you have constant, severe abdominal pain, contact your care provider immediately . Although uncommon, this can be another sign of placental abruption. If you also have a fever and vaginal dischar ge along with the pain, you may have an infection. Decreased movement It’s normal for the vigor of your baby’ s activities to decrease somewhat during the last few days before birth. It’ s almost as if your baby is resting and storing up ener gy for the big day . But the number of movements shouldn’ t drop a great deal. Decreased frequency of movement may be a signal that something is wrong. To\\n\\n### Response: \\n You can tell if your baby is getting enough movement in the womb by paying attention to the frequency and intensity of their movements. While its normal for the number of movements to decrease somewhat in the last few days before birth, a significant decrease or absence of movement could be a cause for concern. Its important to communicate any changes in your babys movement patterns to your healthcare provider.'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(generated_train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b537c97-527a-442d-ad9f-b32599aac306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/openai.py:241: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/openai.py:898: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'openai' has no attribute 'error'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m llm \u001b[38;5;241m=\u001b[39m OpenAI(model_name\u001b[38;5;241m=\u001b[39mllm_name)\n\u001b[1;32m      3\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen did Virgin Australia start operating?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mget_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectordb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 14\u001b[0m, in \u001b[0;36mget_prediction\u001b[0;34m(llm, vectordb, question)\u001b[0m\n\u001b[1;32m      7\u001b[0m QA_CHAIN_PROMPT_2 \u001b[38;5;241m=\u001b[39m PromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(template_2)\n\u001b[1;32m      8\u001b[0m qa_chain_2 \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[1;32m      9\u001b[0m     llm,\n\u001b[1;32m     10\u001b[0m     retriever\u001b[38;5;241m=\u001b[39mvectordb\u001b[38;5;241m.\u001b[39mas_retriever(),\n\u001b[1;32m     11\u001b[0m     return_source_documents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m     chain_type_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: QA_CHAIN_PROMPT_2}\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m result_p_2\u001b[38;5;241m=\u001b[39m \u001b[43mqa_chain_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# return result_p_2['result']\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result_p_2\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/retrieval_qa/base.py:139\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_documents_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_documents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key: answer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource_documents\u001b[39m\u001b[38;5;124m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/base.py:510\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    506\u001b[0m         _output_key\n\u001b[1;32m    507\u001b[0m     ]\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[0;32m--> 510\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    511\u001b[0m         _output_key\n\u001b[1;32m    512\u001b[0m     ]\n\u001b[1;32m    514\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    516\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m but none were provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m     )\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/combine_documents/base.py:122\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    121\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 122\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/combine_documents/stuff.py:171\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_inputs(docs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/llm.py:298\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    284\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/llm.py:108\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    105\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[1;32m    106\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    107\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m--> 108\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/chains/llm.py:120\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    118\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    128\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    129\u001b[0m     )\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/base.py:507\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    501\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    505\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    506\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/base.py:656\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    642\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    643\u001b[0m         )\n\u001b[1;32m    644\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    646\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         )\n\u001b[1;32m    655\u001b[0m     ]\n\u001b[0;32m--> 656\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    659\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/base.py:544\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    543\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    545\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/base.py:531\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    523\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    530\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 531\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    535\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    539\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    540\u001b[0m         )\n\u001b[1;32m    541\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/openai.py:987\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m messages, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_chat_params(prompts, stop)\n\u001b[1;32m    986\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[0;32m--> 987\u001b[0m full_response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    989\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(full_response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    991\u001b[0m     full_response \u001b[38;5;241m=\u001b[39m full_response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/openai.py:114\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m \u001b[43m_create_retry_decorator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/xzhong/anaconda3/envs/bishwa/lib/python3.11/site-packages/langchain/llms/openai.py:94\u001b[0m, in \u001b[0;36m_create_retry_decorator\u001b[0;34m(llm, run_manager)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_retry_decorator\u001b[39m(\n\u001b[1;32m     86\u001b[0m     llm: Union[BaseOpenAI, OpenAIChat],\n\u001b[1;32m     87\u001b[0m     run_manager: Optional[\n\u001b[1;32m     88\u001b[0m         Union[AsyncCallbackManagerForLLMRun, CallbackManagerForLLMRun]\n\u001b[1;32m     89\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     90\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Callable[[Any], Any]:\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mopenai\u001b[39;00m\n\u001b[1;32m     93\u001b[0m     errors \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m---> 94\u001b[0m         \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror\u001b[49m\u001b[38;5;241m.\u001b[39mTimeout,\n\u001b[1;32m     95\u001b[0m         openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIError,\n\u001b[1;32m     96\u001b[0m         openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mAPIConnectionError,\n\u001b[1;32m     97\u001b[0m         openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mRateLimitError,\n\u001b[1;32m     98\u001b[0m         openai\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mServiceUnavailableError,\n\u001b[1;32m     99\u001b[0m     ]\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m create_base_retry_decorator(\n\u001b[1;32m    101\u001b[0m         error_types\u001b[38;5;241m=\u001b[39merrors, max_retries\u001b[38;5;241m=\u001b[39mllm\u001b[38;5;241m.\u001b[39mmax_retries, run_manager\u001b[38;5;241m=\u001b[39mrun_manager\n\u001b[1;32m    102\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'openai' has no attribute 'error'"
     ]
    }
   ],
   "source": [
    "llm_name='gpt-3.5-turbo'\n",
    "llm = OpenAI(model_name=llm_name)\n",
    "question = \"When did Virgin Australia start operating?\"\n",
    "result = get_prediction(llm, vectordb, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af530c5-3b3b-483a-aba5-048ad3ebb605",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af0c02c-bb76-466d-9c60-908d0e0660cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_name='gpt-3.5-turbo'\n",
    "llm = ChatOpenAI(model_name=llm_name, temperature=0)\n",
    "\n",
    "template_2 = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context.\n",
    "        \\n\\n###Instruction: {question}\n",
    "        \\n\\n### Input: {context}\n",
    "        \\n\\n### Response: \\n\"\"\"\n",
    "\n",
    "QA_CHAIN_PROMPT_2 = PromptTemplate.from_template(template_2)\n",
    "qa_chain_2 = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT_2}\n",
    ")\n",
    "question_p_2 = \"When did Virgin Australia start operating?\"\n",
    "result_p_2= qa_chain_2({\"query\":question_p_2})\n",
    "result_p_2['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74e187f-ae8a-4e2d-801c-2367b557596b",
   "metadata": {},
   "source": [
    "#### Using Mistral with RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98067f87-eb28-4d69-a8b5-86e8d55dd39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFaceHub\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    # trust_remote_code=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    # use_auth_token=True\n",
    ")\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    # add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    "    padding_size=\"left\"\n",
    ")\n",
    "eval_tokenizer.pad_token = eval_tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa7a123-997e-45a7-bb1f-421848361141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "generate_text = transformers.pipeline(\n",
    "    model=base_model, \n",
    "    tokenizer=eval_tokenizer,\n",
    "    return_full_text=True,  # langchain expects the full text\n",
    "    task='text-generation',\n",
    "    # we pass model parameters here too\n",
    "    # stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
    "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
    "    max_new_tokens=512,  # max number of tokens to generate in the output\n",
    "    repetition_penalty=1.1  # without this output begins repeating\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c8136d-bfa3-42a8-b4db-379446a5ebf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "llm = HuggingFacePipeline(pipeline=generate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bac5c85-6f40-4127-81c1-7a6d0c6e29be",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"When did Virgin Australia start operating?\"\n",
    "result = get_prediction(llm, vectordb, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c1118-db4c-440d-a562-01c57b7962db",
   "metadata": {},
   "outputs": [],
   "source": [
    "result['result'].split('\\nresponse:')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec229ce-dd34-4bdb-b0dd-fd40ffcecc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['instruction'] = []\n",
    "output['context'] = []\n",
    "output['response'] = []\n",
    "output['model_output'] = []\n",
    "for data in eval_dataset:\n",
    "    out = get_prediction(llm, vectordb, data['instruction'])\n",
    "    output['instruction'].append(data['instruction'])\n",
    "    output['context'].append(data['context'])\n",
    "    output['response'].append(data['response'])\n",
    "    output['model_output'].append(out['result'].split('Response: \\n')[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b58f44-11eb-45fc-adf9-25e5b7e43775",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = evaluate.load('rouge')\n",
    "results = rouge.compute(predictions=output['model_output'], references=output['response'][0:len(output['model_output'])],\n",
    "                        use_aggregator=True,\n",
    "                        use_stemmer=True,)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be0c19-0912-4e4d-aca8-cac53fc43aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a798c498-07d1-417b-87cb-77fa0b1a43d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
